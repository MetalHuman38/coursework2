{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install skit tensorflow tensorflow_datasets tensorflow opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Name: Babatunde Kalejaiye\n",
    "Student ID: KAL20504783\n",
    "CourseWork: 2\n",
    "Course: Machine Learning\n",
    "Date: 2024-11-22\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### CelebADatasets Class\n",
    "\n",
    "The `CelebADatasets` class is a custom dataset class for handling the CelebA dataset, which is commonly used for facial attribute recognition tasks. This class inherits from `torch.utils.data.Dataset` and provides methods to load and preprocess the dataset.\n",
    "\n",
    "#### Initialization\n",
    "- `__init__(self, image_dir, attr_file, label_col=\"Smiling\", transform=None)`: Initializes the dataset with the directory of images, the path to the attributes file, the label column to use, and any transformations to apply to the images.\n",
    "  - `image_dir` (str): Directory containing all the images.\n",
    "  - `attr_file` (str): Path to the attributes file.\n",
    "  - `label_col` (str): The attribute to use as the label (default is \"Smiling\").\n",
    "  - `transform` (callable, optional): Transformations to apply to the images.\n",
    "\n",
    "#### Methods\n",
    "- `__len__(self)`: Returns the total number of samples in the dataset.\n",
    "- `__getitem__(self, idx)`: Retrieves the image and label at the specified index.\n",
    "  - `idx` (int): Index of the sample to retrieve.\n",
    "  - Returns a tuple `(image, label)` where `image` is the transformed image and `label` is the corresponding label.\n",
    "\n",
    "#### Attributes\n",
    "- `image_dir`: Directory containing the images.\n",
    "- `transform`: Transformations to apply to the images.\n",
    "- `attributes`: DataFrame containing the attributes loaded from the attributes file.\n",
    "- `data`: DataFrame containing the image IDs and the selected label column.\n",
    "\n",
    "This class handles loading the attributes, selecting the specified label column, and applying transformations to the images. It raises a `ValueError` if the specified label column is not found in the attributes file.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CelebADatasets(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for CelebA.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, attr_file, label_col=\"Smiling\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all images.\n",
    "            attr_file (str): Path to attributes file.\n",
    "            label_col (str): Attribute to use as label.\n",
    "            transform (callable, optional): Transformations to apply to images.\n",
    "        \"\"\"\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load attributes\n",
    "        self.attributes = pd.read_csv(attr_file, sep=',', header=0)\n",
    "        self.attributes.rename(columns={\"Unnamed: 0\": \"image_id\"}, inplace=True)\n",
    "\n",
    "        # Select the label column\n",
    "        if label_col not in self.attributes.columns:\n",
    "            raise ValueError(f\"Label column '{label_col}' not found in attributes.\")\n",
    "        self.data = self.attributes[[\"image_id\", label_col]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx][\"image_id\"]\n",
    "        label = int((self.data.iloc[idx][\"Smiling\"] + 1) / 2)\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Below method loads the attributes file and read with pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the attributes file\n",
    "attr_file = \"img_align_celeba/list_attr_celeba.txt\"\n",
    "\n",
    "# Load the attributes\n",
    "attributes = pd.read_csv(attr_file, sep=',', header=0)\n",
    "\n",
    "# Print the first few rows and column names\n",
    "print(attributes.head())\n",
    "print(\"Columns in attributes:\", attributes.columns)\n",
    "print(\"Columns in attributes:\", attributes.columns.tolist())\n",
    "print(\"Is 'Smiling' in columns?\", \"Smiling\" in attributes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Create a table with column names\n",
    "table = PrettyTable()\n",
    "table.field_names = attributes.columns.tolist()\n",
    "\n",
    "# Add the first few rows\n",
    "for i in range(5):\n",
    "    table.add_row(attributes.iloc[i].tolist())\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CelebA Dataset Preprocessing and DataLoader Setup\n",
    "\n",
    "This notebook demonstrates the setup for preprocessing the CelebA dataset and creating train/test data loaders using PyTorch and associated libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from matplotlib import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Paths\n",
    "dataset_root = \"img_align_celeba\"\n",
    "image_dir = \"img_align_celeba\"\n",
    "attr_file = os.path.join(image_dir, \"list_attr_celeba.txt\")\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip 50% of the time\n",
    "    transforms.RandomRotation(15),          # Random rotation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load full dataset\n",
    "full_dataset = CelebADatasets(image_dir, attr_file, label_col=\"Smiling\", transform=transform)\n",
    "\n",
    "# Split into train/test\n",
    "train_indices, test_indices = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(full_dataset, test_indices)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### This below class SimpleCNN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Network (CNN)\n",
    "\n",
    "This section introduces a basic CNN model implemented using PyTorch. The model is designed to process the CelebA dataset prepared in the previous implementation and predict a binary classification label (e.g., \"Smiling\" or \"Not Smiling\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        # self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pretrained ResNet18 for Binary Classification\n",
    "\n",
    "This section demonstrates how to use a pretrained ResNet18 model from `torchvision` to perform binary classification. This builds upon the previous models by leveraging transfer learning, which can significantly improve performance on tasks with limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load ResNet18 with the correct weights\n",
    "weights = ResNet18_Weights.IMAGENET1K_V1  # Use the most relevant pre-trained weights\n",
    "model = resnet18(weights=weights)\n",
    "\n",
    "# Modify the final fully connected layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Freeze all layers except the last fully connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the last fully connected layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating the Model\n",
    "\n",
    "This section implements the training and testing process for the model, where the \"magic\" of machine learning happens. The model learns patterns from the training data and evaluates its performance on the test data. Additionally, the trained model is saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze()  # Match labels' shape\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Test loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.float().to(device)\n",
    "        outputs = model(images).squeeze()\n",
    "        predicted = torch.sigmoid(outputs) > 0.5\n",
    "        correct += (predicted.int() == labels.int()).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating and Displaying Predictions with Confidence\n",
    "\n",
    "This section focuses on evaluating the model's predictions on the test dataset and visualizing the results in a tabular format using the `PrettyTable` library. The table includes the image ID, true label, predicted label, and the confidence of the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Assuming `attributes` contains the image information\n",
    "# Assuming `test_indices` is a list of indices corresponding to the test set\n",
    "\n",
    "# Create a table with column names\n",
    "result_table = PrettyTable()\n",
    "result_table.field_names = [\"Image ID\", \"True Label\", \"Predicted Label\", \"Confidence\"]\n",
    "\n",
    "# Ground truth and predictions\n",
    "y_true = []  # Ground truth labels\n",
    "y_pred = []  # Model predictions\n",
    "logits = []  # Raw logits from the model\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).squeeze()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).int()\n",
    "\n",
    "        # Append data\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        logits.extend(outputs.cpu().numpy())\n",
    "\n",
    "# Populate the table\n",
    "for idx, (true_label, pred_label, logit) in enumerate(zip(y_true, y_pred, logits)):\n",
    "    # Get the image ID from the attributes file\n",
    "    image_id = attributes.iloc[test_indices[idx]][\"image_id\"]\n",
    "\n",
    "    # Convert the logit to a confidence score (between 0 and 1)\n",
    "    confidence = torch.sigmoid(torch.tensor(logit)).item()\n",
    "\n",
    "    # Add a row to the table\n",
    "    result_table.add_row([\n",
    "        image_id,\n",
    "        \"Smiling\" if true_label == 1 else \"Not Smiling\",\n",
    "        \"Smiling\" if pred_label == 1 else \"Not Smiling\",\n",
    "        f\"{confidence:.2f}\"  # Format confidence to 2 decimal places\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Model Errors: False Positives and False Negatives\n",
    "\n",
    "This section focuses on analyzing and visualizing the errors made by the model during testing. Specifically, it identifies and displays images that were incorrectly classified as either false positives or false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create lists to store indices\n",
    "false_positive_indices = []\n",
    "false_negative_indices = []\n",
    "images_store = []\n",
    "\n",
    "# Analyze errors\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).squeeze()\n",
    "        predicted = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "        # Convert to CPU for easier indexing\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            if predicted[i] == 1 and labels[i] == 0:  # False Positive\n",
    "                false_positive_indices.append((batch_idx, i))\n",
    "            elif predicted[i] == 0 and labels[i] == 1:  # False Negative\n",
    "                false_negative_indices.append((batch_idx, i))\n",
    "\n",
    "def visualize_errors(indices, dataset, title, num_images=16, cols=8):\n",
    "    \"\"\"\n",
    "    Visualize error images (false positives or negatives) from the dataset.\n",
    "\n",
    "    Args:\n",
    "    - indices: List of (batch_idx, image_idx) tuples for errors.\n",
    "    - dataset: Dataset object (e.g., test_dataset).\n",
    "    - title: Title for the visualization.\n",
    "    - num_images: Number of images to visualize.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = (num_images + cols - 1) // cols  # Calculate number of rows\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 10))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # Flatten axes for easier indexing\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (batch_idx, image_idx) in enumerate(indices[:num_images]):\n",
    "        # Get the image from the dataset\n",
    "        image, _ = dataset[batch_idx * test_loader.batch_size + image_idx]\n",
    "\n",
    "        # Convert tensor to a displayable format\n",
    "        image = image.permute(1, 2, 0).numpy() * 0.5 + 0.5  # Unnormalize\n",
    "\n",
    "        # Display image in the corresponding subplot\n",
    "        axes[idx].imshow(image)\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    # Turn off any unused axes\n",
    "    for ax in axes[len(indices[:num_images]):]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit title\n",
    "    plt.show()\n",
    "\n",
    "visualize_errors(false_positive_indices, test_dataset, \"False Positives\")\n",
    "visualize_errors(false_negative_indices, test_dataset, \"False Negatives\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Distribution Analysis\n",
    "\n",
    "This section analyzes the distribution of model confidence scores for the test dataset, broken down by the true labels (\"Smiling\" and \"Not Smiling\"). This provides insight into how confident the model is in its predictions and whether there is any overlap in its confidence for the two classes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collect logits and labels\n",
    "logits = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).squeeze()\n",
    "        logits.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "logits = np.array(logits)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Plot confidence distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(logits[all_labels == 1], bins=30, alpha=0.7, label=\"Smiling (True)\")\n",
    "plt.hist(logits[all_labels == 0], bins=30, alpha=0.7, label=\"Not Smiling (True)\")\n",
    "plt.xlabel(\"Logit (Confidence)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title(\"Confidence Distribution by True Class\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.sortby = \"Confidence\"\n",
    "result_table.reversesort = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Classification Results to CSV\n",
    "\n",
    "This section exports the model's classification results, including image IDs, true labels, predicted labels, and confidence scores, into a CSV file. This allows for further analysis or sharing of results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_csv = pd.DataFrame([\n",
    "    [image_id, true_label, pred_label, confidence]\n",
    "    for image_id, true_label, pred_label, confidence in zip(\n",
    "        attributes.iloc[test_indices][\"image_id\"],\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        [torch.sigmoid(torch.tensor(logit)).item() for logit in logits]\n",
    "    )\n",
    "], columns=[\"Image ID\", \"True Label\", \"Predicted Label\", \"Confidence\"])\n",
    "\n",
    "result_table_csv.to_csv(\"classification_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking and Displaying Unique Classification Results\n",
    "\n",
    "This section demonstrates how to track unique classification results for each image in a test dataset using a dictionary and display them in a structured table format with a focus on highlighting errors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dictionary to track unique entries\n",
    "unique_results = {}\n",
    "\n",
    "for idx, (true_label, pred_label, logit) in enumerate(zip(y_true, y_pred, logits)):\n",
    "    image_id = attributes.iloc[test_indices[idx]][\"image_id\"]\n",
    "    confidence = torch.sigmoid(torch.tensor(logit)).item()\n",
    "\n",
    "    # Store the result in the dictionary, keyed by image_id\n",
    "    unique_results[image_id] = {\n",
    "        \"True Label\": \"Smiling\" if true_label == 1 else \"Not Smiling\",\n",
    "        \"Predicted Label\": \"Smiling\" if pred_label == 1 else \"Not Smiling\",\n",
    "        \"Confidence\": f\"{confidence:.2f}\" + (\"*\" if true_label != pred_label else \"\")\n",
    "    }\n",
    "\n",
    "# Create a PrettyTable from the unique results\n",
    "result_table = PrettyTable()\n",
    "result_table.field_names = [\"Image ID\", \"True Label\", \"Predicted Label\", \"Confidence\"]\n",
    "\n",
    "for image_id, values in unique_results.items():\n",
    "    result_table.add_row([image_id, values[\"True Label\"], values[\"Predicted Label\"], values[\"Confidence\"]])\n",
    "\n",
    "# Display the table\n",
    "print(result_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
